/Users/lauraschulz/Documents/IBM/instruct-lab/.conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
  from .autonotebook import tqdm as notebook_tqdm
Fetching 9 files: 100%|██████████| 9/9 [03:32<00:00, 23.56s/it]
Downloading detection model, please wait. This may take several minutes depending upon your network connection.
Progress: |██████████████████████████████████████████████████| 100.0% Complete
Downloading recognition model, please wait. This may take several minutes depending upon your network connection.
Progress: |██████████████████████████████████████████████████| 100.0% CompleteTutorial

## Contributing knowledge to the open source Granite models and LLMs using the InstructLab UI

A hands-on guide for getting started using the InstructLab UI and adding knowledge to the Granite model

<!-- image -->

<!-- image -->

<!-- image -->

<!-- image -->

By Ahmed Azraq 17 October 2024

Large Language Models (LLMs) have immense potential, but they come with challenges like the need for high-quality training data, specialized skills, and extensive computing resources. Forking and retraining these models can be time-consuming and costly. That's where steps in! This opensource project leverages community contributions to enhance LLMs, offering a more efficient and collaborative approach to generative AI. InstructLab

## InstructLab

is a communitybased approach

The "lab" in InstructLab stands for Large-Scale Alignment for ChatBots,

to building truly open-source LLMs. The InstructLab

which is detailed in the research paper. LAB: Large-Scale Alignment for ChatBots

community model will be updated with a periodic release cycle for models and data shared regularly on . Learn more about what InstructLab is and why developers need it in this article on IBM Developer. Hugging Face

In my previous tutorial Contributing knowledge

to the open source Granite model using InstructLab , I showed how to set up you local environment, and run the entire process locally of fine-tuning and contributing to open-source LLM; for example (IBM Granite). The end to end process included updating the taxonomy and dealing with YAML, which requires expertise with YAML, and also required familiarity with GitHub pull request process.

In this tutorial, you learn how to accomplish the same goal of fine-tuning open-source LLM using which removes the complexity typically associated with manually handling YAML structures while updating the taxonomy, validation rules and no need for familiarity with GitHub pull request process. InstructLab UI

The InstructLab User Interface (UI) allows you to easily contribute knowledge or skills to the InstructLab taxonomy repository without worrying about YAML structure and the different validation rules.

The InstructLab UI provides an intuitive

interface that simplifies the process of contribution to InstructLab taxonomy. Whether you're contributing knowledge (such as facts, data, or references) or skills (which teach the model how to perform specific tasks), the InstructLab UI removes the complexity typically associated with manually handling YAML structures, validation rules, and familiarity with the GitHub pull request process. The InstructLab UI enables users to focus on enhancing the open source large language model's capabilities without needing to worry about the tools and processes and allow them to focus on the content of the

contribution. This intuitive UI can be used by Products
                   |                 |                                                                                                                                                                  |        | Topics                                  |         |

taxonomy

Step 4. Train the model locally with the new knowledge

Step 5. Optionally, contribute the knowledge to InstructLab

Summary and next steps

Acknowle…

## Steps

## Step 1. Chat with the model

In this section, you are going to chat with the current large language model to see if the model is already trained on the knowledge or skill that you would like to contribute or not.

InstructLab UI allows you to chat with these two models (as of the time of writing this tutorial) served on the Cloud:

: Open-source IBM Granite model. Granite-7b

: Derivative model from Mistral-7b, trained with the LAB method. Merlinite-7b

Additionally, you can chat with your own served model by adding its endpoint in "Custom Model Endpoints".

To chat with the model, follow these steps:

1. Open InstructLab UI

.

2. Navigate to Playground > Chat .

3. In the Model Selector , select the model you'd like to chat with. Select Granite-7B from the drop-down.

What is InstructLab and why do developers need it?

Trials

Try watso nx.ai

## Interested in generative AI?

Learn generative AI skills

<!-- image -->

4. Type What is IBM Granite , and then click Send .

5. The IBM Granite model is not yet trained on the knowledge about IBM Granite. The existing model didn't identify that IBM Granite is a foundation model. The model in this case is hallucinating, so you may receive different results.

<!-- image -->

In the next steps, you learn how to fine-tune an open-source LLM (IBM Granite) with additional knowledge.

## Step 2. Prepare the new knowledge as a Markdown file

In this section, you'll create a markdown file including the new knowledge. This is the source knowledge file: . IBM Granite from Wikipedia

1. Convert the Wikipedia article into markdown format. Try to make the MD file readable, clean, and do not use Markdown table formats.

2. Save the Markdown file locally with extension .md , for example: IBMGranite.md .

## Step 3. Add the new knowledge to the taxonomy

InstructLab uses a synthetic-data-based alignment tuning method for LLMs. InstructLab is driven by carefully created taxonomies, built into a taxonomy tree. The taxonomy allows you to train models tuned with your additional skills or knowledge.

InstructLab UI does extensive validation to make sure that the submissions confirms to all the standards.

1. In InstructLab UI, navigate to Contribute > Knowledge .

<!-- image -->

2. Enter the author information. This information is required when submitting your change as a Pull-Request on the upstream for GitHub DCO (Developer Certificate of Origin). Ensure that you use the same name and email address that is associated with your GitHub account. In the email field, try to put an invalid email format, and note that InstructLab UI prevents that.

<!-- image -->

3. Enter more details about the knowledge being contributed, including a submission summary, domain, and document outline. In the first field, try to put a longer description, and notice that InstructLab UI limits it to only 60 characters.

<!-- image -->

4. Specify the location in the taxonomy tree of your additional knowledge. You can navigate to the target location using the list in dropdown. Once you choose your target directory, please add the name of your subdirectory where you want your contribution to be stored. Make sure you use the subdirectory name that is relevant to your contribution. For example, specify technology/large_language_models/granite . This means that the output yaml file qna.yaml and the attributions file attribution.txt will be stored in this location in the taxonomy tree.

<!-- image -->

5. To contribute knowledge, you need at least 5 seed examples that will be used by the teacher model for synthetic data generation. Each example must have a context from the associated markdown document, with at least 3 sample question and answer pairs .

<!-- image -->

6. Fill the first seed example with context and associated 3 sample question and answer pairs as shown below.

<!-- image -->

7. Similarly, fill the remaining 4 seed examples.

8. In the document information, upload the reference grounded knowledge that you prepared earlier as Markdown by clicking the Upload button and choosing the Markdown file, and then click Submit Files .

<!-- image -->

9. Fill the attribution details with the actual link including the knowledge, title of the knowledge, the specific revision used, license details of the knowledge being contributed and actual knowledge author.

<!-- image -->

10. Download the generated qna.yaml and attribution.txt by clicking Download .

<!-- image -->

## Step 4. Train the model locally with the new knowledge

In this section, you will use InstructLab to train the model locally with the new knowledge. Check out this tutorial Contributing knowledge to the open source Granite model using InstructLab for more details on how to set up you local environment and run the entire process locally. This section assumes you already have completed this other tutorial.

1. Navigate to InstructLab directory and clone the taxonomy

git clone

https://github.com/instructlab/taxonomy

Show more

2. Create a new folder inside taxonomy to add the new knowledge.

mkdir

-p taxonomy/knowledge/technology/large_language_model/granite

Show more

3. Move the downloaded qna.yaml and attribution.txt inside the newly created folder.

4. Verify that the InstructLab detects the new

taxonomy change you created and has a valid syntax. You should not worry a lot here since InstructLab UI ensured that the generated taxonomy is valid.

ilab taxonomy diff

Show more

#Output Taxonomy in

./taxonomy is valid :)

Show more

5. Generate synthetic training data.

ilab data generate

Show more

6. Train the model. Keep the default parameters if you want to train Granite model as a student model. If you want to train merlinite model instead, pass the argument --model-path instructlab/merlinite-7b-lab .

ilab model train --pipeline simple

Show more

7. If you want to run this locally on your Mac,

knowledge/technology/large_language_model/granite/qna.yaml

convert the newly trained model into quantized model . GGUF format

ilab model convert --adapter-file

Show more

8. Serve the newly trained model.

Show more

9. Chat with the newly trained model in a new terminal window.

ilab model chat

Show more

Now, the moment of the truth. Ask the new model question about the new knowledge contributed. In the same terminal window, type an inquiry to the model What is IBM Granite? as shown below.

<!-- image -->

Notice that the trained model now identifies IBM Granite as a foundation model developed by IBM Research. The model may have some hallucinations because of the use of a quantized model running on a laptop with low resources for data generation, training, and inferencing. However, the improved accuracy of the response demonstrate that the IBM Granite model now is successfully trained on your knowledge contribution about "IBM Granite".

## Step 5. Optionally, contribute the knowledge to InstructLab

In this final section, you learn how to contribute knowledge to InstructLab taxonomy repo to improve large language models. This an optional step, you can just read through it to understand the process so that you can follow it when you have actual knowledge or skills that you would like to contribute to InstructLab.

You will use InstructLab UI to accomplish this step.

1. Go back to Instruct Lab UI browser window.

2. Click Submit to automatically open a Pull Request on the InstructLab taxonomy repository. You should receive a successful message as shown in the below snapshot after the successful submission.

<!-- image -->

3. Click View your pull request to view the Pull Request. Check out this sample Pull-Request

.

## Summary and next steps

In this tutorial, you learned how to contribute to fine-tuning and improving LLMs (Large Language Models) such as the IBM Granite models using InstructLab UI.

Now that you've seen the power of InstructLab UI, check out , which brings together the open source Granite family of LLMs, the InstructLab model Red Hat Enterprise Linux AI

alignment tools, a boatable image of Red Hat Enterprise Linux, including popular AI libraries such as PyTorch, and enterprise-grade technical support and open source assurance legal protections. Then, you can scale your AI workflows with and begin using IBM watsonx.ai , which provides additional capabilities for enterprise AI development, model governance, enterprise workflows around advanced data ingestion, data lineage, governance, and model evaluation capabilities. Red Hat OpenShift AI

## Acknowledgments

This tutorial is produced as part of an IBM Open Innovation Community initiative.

The author appreciates the efforts of Anil Vishnoi (Principal Software Engineer, Red Hat), Susan Malaika (Senior Technical Staff Member, IBM), and Suhas Kashyap (Senior Product Manager, InstructLab for watsonx.ai, IBM) for their guidance and expertise in reviewing and contributing to this tutorial.

<!-- image -->

<!-- image -->

<!-- image -->
